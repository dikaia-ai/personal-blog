<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Intro to OCTIS | Kyle's Blog</title>
<meta name=keywords content>
<meta name=description content="Optimizing and Comparing Topic Models is Simple!">
<meta name=author content="Kyle McLester">
<link rel=canonical href=https://kmclester.com/posts/octis/octis/>
<meta name=google-site-verification content="G-06XBWZ9STS">
<link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://kmclester.com/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://kmclester.com/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://kmclester.com/favicon-32x32.png>
<link rel=apple-touch-icon href=https://kmclester.com/apple-touch-icon.png>
<link rel=mask-icon href=https://kmclester.com/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-06XBWZ9STS"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-06XBWZ9STS',{anonymize_ip:!1})}</script>
<meta property="og:title" content="Intro to OCTIS">
<meta property="og:description" content="Optimizing and Comparing Topic Models is Simple!">
<meta property="og:type" content="article">
<meta property="og:url" content="https://kmclester.com/posts/octis/octis/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2022-02-09T00:00:00+00:00">
<meta property="article:modified_time" content="2022-02-09T00:00:00+00:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Intro to OCTIS">
<meta name=twitter:description content="Optimizing and Comparing Topic Models is Simple!">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://kmclester.com/posts/"},{"@type":"ListItem","position":2,"name":"Intro to OCTIS","item":"https://kmclester.com/posts/octis/octis/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Intro to OCTIS","name":"Intro to OCTIS","description":"Optimizing and Comparing Topic Models is Simple!","keywords":[],"articleBody":"OCTIS GitHub \nOCTIS or “Comparing and Optimizing Topic Models is Simple” was created by a team in Italy to assist with training, analyzing, and comparing topic models (Terragni et al. 2021). They intend for OCTIS to be used by researchers to aid in comparing models of interest; which are scored using benchmark datasets and well-known evalutation metrics. Assuming wide adoption, it would make comparing research teams results' much easier as everyone would be using the same testing methodology.\nWhat is Topic Modeling? If you have never heard of or worked with topic modeling, have no fear; the concept is pretty intuitive. The general premise behind topic models are that they are statistical methods that aim to extract the hidden topics underlying a collection of documents. Said differently, they allow us to get a “birds eye view” of a documents content. For example, if we were performing topic modeling on the linked research paper, the model may return “octis framework optimize evaluation hyperparameter”. As I mentioned, this gives us an idea of what the document is about without having to read the entire paper.\nA Brief History An early version of topic modeling was originally described in 1998 (Papadimitriou et al. 1998). Another was created by Thomas Hofmann in 1999 called probabilistic latent semantic analysis or “PLSA” (Hofmann 1999). Shortly thereafter, Latent Dirichlet allocation (LDA) was created as a generalization of PLSA - this is generally accepted as the most common topic model currently in use (Blei, Ng, and Jordan 2003).\nLDA is an unsupervised approach that allows us to find latent or “hidden” themes in a collection of documents. This process assumes each document is a “bag of words” and that each word/document falls into one of k topics. We’re able to use the following probabilities: P(topic t | document d) and P(word w | topic t) to surmise the appropriate topic associated with each document. LDA also uses Bayesian principles in the form of generating prior predictive probabilities and applying Bayesian updating to iteratively calculate posterior probabilities - once some threshold is met, the algorithm returns the list of topics and associated words.\nAs I mentioned, LDA is one of the most popular options and is great for what it is but it experiences one major pitfall – context. LDA and topic models as a whole began to fall out of favor with scientists around 2012 due to the introduction of neural based approaches (Ruder 2021). These approaches include word embeddings (word2vec, glove, etc), sequence-to-sequence models, attention (transform) models, and pre-trained language models (BERT, Huggingface, etc). A nueral based approach allows models to account for the context in which a word is spoken/written - they can alter their vector representation based on surrounding words, part of speech, etc.\nFortunately, topic models have recently recieved a neural “facelift” as well. Several new methods such as Neural LDA, Embedded Topic Models (ETM), and Contextualized Topic Models (CTM) have all been developed within the last two to five years (Terragni et al. 2021). The team behind OCTIS hope to combine both modern and traditional techniques into a unified framework - such that researchers can compare the accuracy of models against a standardized baseline. The intention is not to determine which model is the all-around king of models, rather, it is to test which model would be the best for a given scenario. Neural approaches are all the rave right now but occassionally it is wise to brush off the traditional methods as they may be faster, more efficient, or achieve 95% of the desired response with minimal effort. It is also a good idea to understand the old way of doing things so you can better appreciate what newer methods have to offer.\ntl;dr LDA is great; people think neural nets are cool; new isn’t always better but sometimes it is; OCTIS wants to make comparing them easier; models are just tools\nLearning is a Process OCTIS follows the general framework shown below. These pipelines are setup for ease of use but also repeatability. Understanding this workflow is key to being successful with OCTIS.\nPre-processing First, we import a raw dataset and pass it to the pre-processing pipeline. This pipeline can convert all text to lowercase, remove punctuation, lemmatization, remove stop words (a, and, the, etc.), remove unfrequent and most frequent words based on a given threshold, and remove documents with less words than some threshold value. It is important to remember that just because a utility is included, does not mean you have to use it. Not every document requires the aforementioned operations and it’s up to the researchers discression to determine the best course of action.\nOCTIS includes several tests datasets:\n   Dataset Domain # Docs Avg # words in docs # Unique words     20 News-groups Forum posts 16309 48 1612   BBC News News 2225 150 3106   M10 Scientific Papers 8355 6 1696   DBLP Scientific Papers 54595 5 1513    Topic Modeling OCTIS includes a variety of off-the-shelf topic models, including both classical and neural approaches. The included models are:\n Latent Dirichlet Allocation (LDA) Non-negative Matrix Factorization (NMF) Latent Semantic Analysis (LSI) Hierarchical Dirichlet Process (HDP) Neural LDA Product-of-Experts LDA (ProdLDA) Embedded Topic Models (ETM) Contexualized Topic Models (CTM)  Topic models are effectively a black-box. They take a dataset as input and a set of hyperparameters and return the top-t topic words, the document-topic distributions, and the topic-word distribution in a specified format.\nEvaluation metrics The evaluation metrics can be used in two ways:\n An objective which is targeted by the Bayesian Optimization strategy A metric in which to monitor the behavior of a topic model while the model is optimized on a different objective  Performance of the topic model can be evaluated using the following metrics:\n Topic coherence metrics Topic significance metrics Diversity metrics Classification metrics  OCTIS provides 10 evaluation metrics directly from their web dashboard and then 13 additional metrics can be accessed through the python library.\nHyper-parameter Optimization If any of the available hyper-parameters are selected to be optimized (for a given evaluation metric), the Bayesian Optimization engine explores the search space to determine the optimal settings. These optimal settings are based on a desired threshold set for the selected evaluation metric. The team behind OCTIS realized that the performance estimated by these metrics can be subject to noise, so they decided the objective function should be computed as the median of n-model runs, using the same hyper-paramter configuration.\nIn this case, Bayesian Optimization is a sequential model-based optimization strategy for black-box functions (topic models). The general idea is to use all of the model’s configurations evaluated so far to best approximate the value of the selected performance metric and then select a new promising configuration to evaluate (Terragni et al. 2021).\n“The approximation is provided by a probabilistic surrogate model, which describes the prior belief over the objective function using the observed configurations. The next configuration to evaluate is selected through the optimization of an acquisition function, which leverages the uncertainty in the posterior to guide the exploration.” (Terragni et al. 2021)\n OCTIS in Action LDA Example  # Import dependencies from octis.models.LDA import LDA from octis.dataset.dataset import Dataset from octis.evaluation_metrics.diversity_metrics import TopicDiversity from octis.evaluation_metrics.coherence_metrics import Coherence  # Define dataset dataset = Dataset() dataset.fetch_dataset(\"20NewsGroup\")  # Create Model model = LDA(num_topics=20, alpha=0.1)  # Train the model using default partition choice output = model.train_model(dataset) print(*list(output.keys()), sep=\"\\n\") # Print the output identifiers topic-word-matrix topics topic-document-matrix test-topic-document-matrix    # Return the generated topics [' '.join(x) for x in output['topics']] ['woman son church kill body wife people mother leave start', 'government state people law information group issue military make control', 'file image program version application include widget system window server', 'people make give sin day time man life love good', 'list print program port computer printer include work address color', 'people religion make thing belief time christian question point church', 'car good make price engine mile power buy water sell', 'encryption chip clipper key system government technology law year escrow', 'time phone people ground happen start make hear leave put', 'question drive monitor power system apple post newsgroup answer gay', 'game team win play year good player time season make', 'drive card disk run work system driver scsi make chip', 'drug test patient disease doctor medical study problem good card', 'make fire claim people evidence point reason post word case', 'window problem work mode run block bit switch button memory', 'armenian people turkish year genocide population jewish village greek war', 'key mail send number message post info call reply company', 'homosexual man sex homosexuality male sexual cap make pen show', 'gun law weapon people firearm car good death pay kill', 'space launch system cost mission satellite orbit solar make year']    # Initialize performance metric npmi = Coherence(texts=dataset.get_corpus(), topk=10, measure='c_npmi') # Initialize performance metric topic_diversity = TopicDiversity(topk=10)  # Retrieve metric scores topic_diversity_score = topic_diversity.score(output) print(f'Topic diversity: {str(topic_diversity_score)}') npmi_score = npmi.score(output) print(f'Coherence: {str(npmi_score)}') Topic diversity: 0.725   Coherence: 0.07167448455491789    NOTE: For a neural-based example, see the Google Colab  notebook provided by OCTIS.\n Sources Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3 (Jan): 993–1022.\n Hofmann, Thomas. 1999. “Probabilistic Latent Semantic Indexing.” In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 50–57. SIGIR ‘99. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/312624.312649 .\n Papadimitriou, Christos H., Hisao Tamaki, Prabhakar Raghavan, and Santosh Vempala. 1998. “Latent Semantic Indexing: A Probabilistic Analysis.” In Proceedings of the Seventeenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, 159–68. PODS ‘98. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/275487.275505 .\n Ruder, Sebastian. 2021. “A Review of the Recent History of Natural Language Processing.” Sebastian Ruder, Sebastian Ruder.\n Terragni, Silvia, Elisabetta Fersini, Bruno Giovanni Galuzzi, Pietro Tropeano, and Antonio Candelieri. 2021. “OCTIS: Comparing and Optimizing Topic Models Is Simple!” In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, 263–70. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.eacl-demos.31 .\n  ","wordCount":"1661","inLanguage":"en","datePublished":"2022-02-09T00:00:00Z","dateModified":"2022-02-09T00:00:00Z","author":{"@type":"Person","name":"Kyle McLester"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kmclester.com/posts/octis/octis/"},"publisher":{"@type":"Organization","name":"Kyle's Blog","logo":{"@type":"ImageObject","url":"https://kmclester.com/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://kmclester.com/ accesskey=h title="Kyle's Blog (Alt + H)">Kyle's Blog</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://kmclester.com/contact/ title=Contact>
<span>Contact</span>
</a>
</li>
<li>
<a href=https://kmclester.com/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://kmclester.com/categories/ title=Categories>
<span>Categories</span>
</a>
</li>
<li>
<a href=https://kmclester.com/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
<li>
<a href=https://kmclester.com/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://kmclester.com/>Home</a>&nbsp;»&nbsp;<a href=https://kmclester.com/posts/>Posts</a></div>
<h1 class=post-title>
Intro to OCTIS
</h1>
<div class=post-meta><span title="2022-02-09 00:00:00 +0000 UTC">February 9, 2022</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Kyle McLester&nbsp;|&nbsp;<a href=https://github.com/kmcleste/personal-blog/tree/main/content/posts/octis/octis.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#what-is-topic-modeling aria-label="What is Topic Modeling?">What is Topic Modeling?</a></li>
<li>
<a href=#a-brief-history aria-label="A Brief History">A Brief History</a><ul>
<li>
<a href=#tldr aria-label=tl;dr>tl;dr</a></li></ul>
</li>
<li>
<a href=#learning-is-a-process aria-label="Learning is a Process">Learning is a Process</a><ul>
<li>
<a href=#pre-processing aria-label=Pre-processing>Pre-processing</a></li>
<li>
<a href=#topic-modeling aria-label="Topic Modeling">Topic Modeling</a></li>
<li>
<a href=#evaluation-metrics aria-label="Evaluation metrics">Evaluation metrics</a></li>
<li>
<a href=#hyper-parameter-optimization aria-label="Hyper-parameter Optimization">Hyper-parameter Optimization</a></li></ul>
</li>
<li>
<a href=#octis-in-action aria-label="OCTIS in Action">OCTIS in Action</a></li>
<li>
<a href=#sources aria-label=Sources>Sources</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><p><strong><a href=https://github.com/MIND-Lab/OCTIS target=_blank>
OCTIS GitHub
</a></strong></p>
<p>OCTIS or &ldquo;Comparing and Optimizing Topic Models is Simple&rdquo; was created
by a team in Italy to assist with training, analyzing, and comparing
topic models (Terragni et al. 2021). They intend for OCTIS to be used by
researchers to aid in comparing models of interest; which are scored
using benchmark datasets and well-known evalutation metrics. Assuming
wide adoption, it would make comparing research teams results' much
easier as everyone would be using the same testing methodology.</p>
<h2 id=what-is-topic-modeling>What is Topic Modeling?<a hidden class=anchor aria-hidden=true href=#what-is-topic-modeling>#</a></h2>
<p>If you have never heard of or worked with topic modeling, have no fear;
the concept is pretty intuitive. The general premise behind topic models
are that they are statistical methods that aim to extract the hidden
topics underlying a collection of documents. Said differently, they
allow us to get a &ldquo;birds eye view&rdquo; of a documents content. For example,
if we were performing topic modeling on the linked research paper, the
model may return &ldquo;octis framework optimize evaluation hyperparameter&rdquo;.
As I mentioned, this gives us an idea of what the document is about
without having to read the entire paper.</p>
<h2 id=a-brief-history>A Brief History<a hidden class=anchor aria-hidden=true href=#a-brief-history>#</a></h2>
<p>An early version of topic modeling was originally described in 1998
(Papadimitriou et al. 1998). Another was created by Thomas Hofmann in
1999 called <em>probabilistic latent semantic analysis</em> or &ldquo;PLSA&rdquo; (Hofmann
1999). Shortly thereafter, Latent Dirichlet allocation (LDA) was created
as a generalization of PLSA - this is generally accepted as the most
common topic model currently in use (Blei, Ng, and Jordan 2003).</p>
<p>LDA is an unsupervised approach that allows us to find latent or
&ldquo;hidden&rdquo; themes in a collection of documents. This process assumes each
document is a &ldquo;bag of words&rdquo; and that each word/document falls into one
of <strong>k</strong> topics. We&rsquo;re able to use the following probabilities:
<strong>P(topic <em>t</em> | document <em>d</em>)</strong> and <strong>P(word <em>w</em> | topic <em>t</em>)</strong> to
surmise the appropriate topic associated with each document. LDA also
uses Bayesian principles in the form of generating prior predictive
probabilities and applying Bayesian updating to iteratively calculate
posterior probabilities - once some threshold is met, the algorithm
returns the list of topics and associated words.</p>
<p>As I mentioned, LDA is one of the most popular options and is great for
what it is but it experiences one major pitfall &ndash; context. LDA and
topic models as a whole began to fall out of favor with scientists
around 2012 due to the introduction of neural based approaches (Ruder
2021). These approaches include word embeddings (word2vec, glove, etc),
sequence-to-sequence models, attention (transform) models, and
pre-trained language models (BERT, Huggingface, etc). A nueral based
approach allows models to account for the context in which a word is
spoken/written - they can alter their vector representation based on
surrounding words, part of speech, etc.</p>
<p>Fortunately, topic models have recently recieved a neural &ldquo;facelift&rdquo; as
well. Several new methods such as Neural LDA, Embedded Topic Models
(ETM), and Contextualized Topic Models (CTM) have all been developed
within the last two to five years (Terragni et al. 2021). The team
behind OCTIS hope to combine both modern and traditional techniques into
a unified framework - such that researchers can compare the accuracy of
models against a standardized baseline. The intention is not to
determine which model is the all-around king of models, rather, it is to
test which model would be the best for a given scenario. Neural
approaches are all the rave right now but occassionally it is wise to
brush off the traditional methods as they may be faster, more efficient,
or achieve 95% of the desired response with minimal effort. It is also a
good idea to understand the old way of doing things so you can better
appreciate what newer methods have to offer.</p>
<h3 id=tldr>tl;dr<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h3>
<p>LDA is great; people think neural nets are cool; new isn&rsquo;t always better
but sometimes it is; OCTIS wants to make comparing them easier; models
are just tools</p>
<h2 id=learning-is-a-process>Learning is a Process<a hidden class=anchor aria-hidden=true href=#learning-is-a-process>#</a></h2>
<p>OCTIS follows the general framework shown below. These pipelines are
setup for ease of use but also repeatability. Understanding this
workflow is key to being successful with OCTIS.</p>
<p><img loading=lazy src=/octis-workflow.png alt="OCTIS workflow">
</p>
<h3 id=pre-processing>Pre-processing<a hidden class=anchor aria-hidden=true href=#pre-processing>#</a></h3>
<p>First, we import a raw dataset and pass it to the pre-processing
pipeline. This pipeline can convert all text to lowercase, remove
punctuation, lemmatization, remove stop words (a, and, the, etc.),
remove unfrequent and most frequent words based on a given threshold,
and remove documents with less words than some threshold value. It is
important to remember that just because a utility is included, does
<strong>not</strong> mean you have to use it. Not every document requires the
aforementioned operations and it&rsquo;s up to the researchers discression to
determine the best course of action.</p>
<p>OCTIS includes several tests datasets:</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Domain</th>
<th># Docs</th>
<th>Avg # words in docs</th>
<th># Unique words</th>
</tr>
</thead>
<tbody>
<tr>
<td>20 News-groups</td>
<td>Forum posts</td>
<td>16309</td>
<td>48</td>
<td>1612</td>
</tr>
<tr>
<td>BBC News</td>
<td>News</td>
<td>2225</td>
<td>150</td>
<td>3106</td>
</tr>
<tr>
<td>M10</td>
<td>Scientific Papers</td>
<td>8355</td>
<td>6</td>
<td>1696</td>
</tr>
<tr>
<td>DBLP</td>
<td>Scientific Papers</td>
<td>54595</td>
<td>5</td>
<td>1513</td>
</tr>
</tbody>
</table>
<h3 id=topic-modeling>Topic Modeling<a hidden class=anchor aria-hidden=true href=#topic-modeling>#</a></h3>
<p>OCTIS includes a variety of off-the-shelf topic models, including both
classical and neural approaches. The included models are:</p>
<ul>
<li>Latent Dirichlet Allocation (LDA)</li>
<li>Non-negative Matrix Factorization (NMF)</li>
<li>Latent Semantic Analysis (LSI)</li>
<li>Hierarchical Dirichlet Process (HDP)</li>
<li>Neural LDA</li>
<li>Product-of-Experts LDA (ProdLDA)</li>
<li>Embedded Topic Models (ETM)</li>
<li>Contexualized Topic Models (CTM)</li>
</ul>
<p>Topic models are effectively a black-box. They take a dataset as input
and a set of hyperparameters and return the top-t topic words, the
document-topic distributions, and the topic-word distribution in a
specified format.</p>
<h3 id=evaluation-metrics>Evaluation metrics<a hidden class=anchor aria-hidden=true href=#evaluation-metrics>#</a></h3>
<p>The evaluation metrics can be used in two ways:</p>
<ul>
<li>An objective which is targeted by the Bayesian Optimization strategy</li>
<li>A metric in which to monitor the behavior of a topic model while the
model is optimized on a different objective</li>
</ul>
<p>Performance of the topic model can be evaluated using the following
metrics:</p>
<ul>
<li>Topic coherence metrics</li>
<li>Topic significance metrics</li>
<li>Diversity metrics</li>
<li>Classification metrics</li>
</ul>
<p>OCTIS provides 10 evaluation metrics directly from their web dashboard
and then 13 additional metrics can be accessed through the python
library.</p>
<h3 id=hyper-parameter-optimization>Hyper-parameter Optimization<a hidden class=anchor aria-hidden=true href=#hyper-parameter-optimization>#</a></h3>
<p>If any of the available hyper-parameters are selected to be optimized
(for a given evaluation metric), the Bayesian Optimization engine
explores the search space to determine the optimal settings. These
optimal settings are based on a desired threshold set for the selected
evaluation metric. The team behind OCTIS realized that the performance
estimated by these metrics can be subject to noise, so they decided the
objective function should be computed as the median of n-model runs,
using the same hyper-paramter configuration.</p>
<p>In this case, Bayesian Optimization is a sequential model-based
optimization strategy for black-box functions (topic models). The
general idea is to use all of the model&rsquo;s configurations evaluated so
far to best approximate the value of the selected performance metric and
then select a new promising configuration to evaluate (Terragni et al.
2021).</p>
<p>&ldquo;The approximation is provided by a probabilistic surrogate model, which
describes the prior belief over the objective function using the
observed configurations. The next configuration to evaluate is selected
through the optimization of an acquisition function, which leverages the
uncertainty in the posterior to guide the exploration.&rdquo; (Terragni et al.
2021)</p>
<p><img loading=lazy src=/octis-optimization.png alt="Comparison between OCTIS and well known topic modeling
libraries">
</p>
<hr>
<h2 id=octis-in-action>OCTIS in Action<a hidden class=anchor aria-hidden=true href=#octis-in-action>#</a></h2>
<h3 id=lda-examplehttpscolabresearchgooglecomgithubmind-laboctisblobmasterexamplesoctis_lda_training_onlyipynb><a href=https://colab.research.google.com/github/MIND-Lab/OCTIS/blob/master/examples/OCTIS_LDA_training_only.ipynb target=_blank>
LDA Example
</a></h3>
<div class=cell execution_count=1>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Import dependencies</span>
<span style=color:#f92672>from</span> octis.models.LDA <span style=color:#f92672>import</span> LDA
<span style=color:#f92672>from</span> octis.dataset.dataset <span style=color:#f92672>import</span> Dataset
<span style=color:#f92672>from</span> octis.evaluation_metrics.diversity_metrics <span style=color:#f92672>import</span> TopicDiversity
<span style=color:#f92672>from</span> octis.evaluation_metrics.coherence_metrics <span style=color:#f92672>import</span> Coherence
</code></pre></div></div>
<div class=cell execution_count=2>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Define dataset</span>
dataset <span style=color:#f92672>=</span> Dataset()
dataset<span style=color:#f92672>.</span>fetch_dataset(<span style=color:#e6db74>&#34;20NewsGroup&#34;</span>)
</code></pre></div></div>
<div class=cell execution_count=3>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Create Model</span>
model <span style=color:#f92672>=</span> LDA(num_topics<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>)
</code></pre></div></div>
<div class=cell execution_count=4>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Train the model using default partition choice</span>
output <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>train_model(dataset)

print(<span style=color:#f92672>*</span>list(output<span style=color:#f92672>.</span>keys()), sep<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>) <span style=color:#75715e># Print the output identifiers</span>
</code></pre></div><div class=cell-output-stdout>
<pre><code>topic-word-matrix
topics
topic-document-matrix
test-topic-document-matrix
</code></pre>
</div>
</div>
<div class=cell execution_count=5>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Return the generated topics</span>
[<span style=color:#e6db74>&#39; &#39;</span><span style=color:#f92672>.</span>join(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> output[<span style=color:#e6db74>&#39;topics&#39;</span>]]
</code></pre></div><div class=cell-output-display execution_count=5>
<pre><code>['woman son church kill body wife people mother leave start',
 'government state people law information group issue military make control',
 'file image program version application include widget system window server',
 'people make give sin day time man life love good',
 'list print program port computer printer include work address color',
 'people religion make thing belief time christian question point church',
 'car good make price engine mile power buy water sell',
 'encryption chip clipper key system government technology law year escrow',
 'time phone people ground happen start make hear leave put',
 'question drive monitor power system apple post newsgroup answer gay',
 'game team win play year good player time season make',
 'drive card disk run work system driver scsi make chip',
 'drug test patient disease doctor medical study problem good card',
 'make fire claim people evidence point reason post word case',
 'window problem work mode run block bit switch button memory',
 'armenian people turkish year genocide population jewish village greek war',
 'key mail send number message post info call reply company',
 'homosexual man sex homosexuality male sexual cap make pen show',
 'gun law weapon people firearm car good death pay kill',
 'space launch system cost mission satellite orbit solar make year']
</code></pre>
</div>
</div>
<div class=cell execution_count=6>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Initialize performance metric</span>
npmi <span style=color:#f92672>=</span> Coherence(texts<span style=color:#f92672>=</span>dataset<span style=color:#f92672>.</span>get_corpus(), topk<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, measure<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;c_npmi&#39;</span>)

<span style=color:#75715e># Initialize performance metric</span>
topic_diversity <span style=color:#f92672>=</span> TopicDiversity(topk<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</code></pre></div></div>
<div class=cell execution_count=7>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Retrieve metric scores</span>
topic_diversity_score <span style=color:#f92672>=</span> topic_diversity<span style=color:#f92672>.</span>score(output)
print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;Topic diversity: </span><span style=color:#e6db74>{</span>str(topic_diversity_score)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)

npmi_score <span style=color:#f92672>=</span> npmi<span style=color:#f92672>.</span>score(output)
print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;Coherence: </span><span style=color:#e6db74>{</span>str(npmi_score)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</code></pre></div><div class=cell-output-stdout>
<pre><code>Topic diversity: 0.725
</code></pre>
</div>
<div class=cell-output-stdout>
<pre><code>Coherence: 0.07167448455491789
</code></pre>
</div>
</div>
<p><strong>NOTE</strong>: For a neural-based example, see the <a href=https://colab.research.google.com/github/MIND-Lab/OCTIS/blob/master/examples/OCTIS_Optimizing_CTM.ipynb target=_blank>
Google
Colab
</a>
notebook provided by OCTIS.</p>
<hr>
<h2 id=sources>Sources<a hidden class=anchor aria-hidden=true href=#sources>#</a></h2>
<div id=refs class="references csl-bib-body hanging-indent">
<div id=ref-blei2003latent class=csl-entry>
<p>Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. &ldquo;Latent
Dirichlet Allocation.&rdquo; <em>Journal of Machine Learning Research</em> 3 (Jan):
993&ndash;1022.</p>
</div>
<div id=ref-10.1145/312624.312649 class=csl-entry>
<p>Hofmann, Thomas. 1999. &ldquo;Probabilistic Latent Semantic Indexing.&rdquo; In
<em>Proceedings of the 22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval</em>, 50&ndash;57. SIGIR &lsquo;99.
New York, NY, USA: Association for Computing Machinery.
<a href=https://doi.org/10.1145/312624.312649 target=_blank>
https://doi.org/10.1145/312624.312649
</a>.</p>
</div>
<div id=ref-10.1145/275487.275505 class=csl-entry>
<p>Papadimitriou, Christos H., Hisao Tamaki, Prabhakar Raghavan, and
Santosh Vempala. 1998. &ldquo;Latent Semantic Indexing: A Probabilistic
Analysis.&rdquo; In <em>Proceedings of the Seventeenth ACM SIGACT-SIGMOD-SIGART
Symposium on Principles of Database Systems</em>, 159&ndash;68. PODS &lsquo;98. New
York, NY, USA: Association for Computing Machinery.
<a href=https://doi.org/10.1145/275487.275505 target=_blank>
https://doi.org/10.1145/275487.275505
</a>.</p>
</div>
<div id=ref-ruder2021a class=csl-entry>
<p>Ruder, Sebastian. 2021. &ldquo;A Review of the Recent History of Natural
Language Processing.&rdquo; Sebastian Ruder, Sebastian Ruder.</p>
</div>
<div id=ref-terragni-etal-2021-octis class=csl-entry>
<p>Terragni, Silvia, Elisabetta Fersini, Bruno Giovanni Galuzzi, Pietro
Tropeano, and Antonio Candelieri. 2021. &ldquo;OCTIS: Comparing and Optimizing
Topic Models Is Simple!&rdquo; In <em>Proceedings of the 16th Conference of the
European Chapter of the Association for Computational Linguistics:
System Demonstrations</em>, 263&ndash;70. Online: Association for Computational
Linguistics. <a href=https://doi.org/10.18653/v1/2021.eacl-demos.31 target=_blank>
https://doi.org/10.18653/v1/2021.eacl-demos.31
</a>.</p>
</div>
</div>
</div>
<footer class=post-footer>
<nav class=paginav>
<a class=prev href=https://kmclester.com/posts/github/git-ssh/>
<span class=title>« Prev Page</span>
<br>
<span>GitHub SSH</span>
</a>
<a class=next href=https://kmclester.com/posts/quarto/how-to-quarto/>
<span class=title>Next Page »</span>
<br>
<span>How to Quarto</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Intro to OCTIS on twitter" href="https://twitter.com/intent/tweet/?text=Intro%20to%20OCTIS&url=https%3a%2f%2fkmclester.com%2fposts%2foctis%2foctis%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Intro to OCTIS on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fkmclester.com%2fposts%2foctis%2foctis%2f&title=Intro%20to%20OCTIS&summary=Intro%20to%20OCTIS&source=https%3a%2f%2fkmclester.com%2fposts%2foctis%2foctis%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Intro to OCTIS on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fkmclester.com%2fposts%2foctis%2foctis%2f&title=Intro%20to%20OCTIS"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Intro to OCTIS on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkmclester.com%2fposts%2foctis%2foctis%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Intro to OCTIS on whatsapp" href="https://api.whatsapp.com/send?text=Intro%20to%20OCTIS%20-%20https%3a%2f%2fkmclester.com%2fposts%2foctis%2foctis%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Intro to OCTIS on telegram" href="https://telegram.me/share/url?text=Intro%20to%20OCTIS&url=https%3a%2f%2fkmclester.com%2fposts%2foctis%2foctis%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://kmclester.com/>Kyle's Blog</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>